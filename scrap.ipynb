{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from supabase import create_client\n",
    "from multiprocessing import Process\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Connection to Supabase\n",
    "url_supabase = os.getenv(\"SUPABASE_URL\")\n",
    "key = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase = create_client(url_supabase, key)\n",
    "\n",
    "# Get the table\n",
    "db_data = supabase.table(\"idx_key_stats\").select(\"\").execute()\n",
    "df_db_data = pd.DataFrame(db_data.data)\n",
    "\n",
    "cols = df_db_data.columns.tolist()\n",
    "\n",
    "# Get symbol data\n",
    "symbol_list = df_db_data['symbol'].tolist()\n",
    "symbol_list\n",
    "\n",
    "# Remove the .JK\n",
    "for i in range (len(symbol_list)):\n",
    "  symbol_list[i] = symbol_list[i].replace(\".JK\", \"\")\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fail scraping from URL: https://www.tradingview.com/symbols/IDX-GOTO/technicals/\n",
      "Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'symbol': 'GOTO', 'technical_rating': None, 'analyst_rating': None}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "from requests_html import AsyncHTMLSession, HTMLSession\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import requests\n",
    "\n",
    "# Scraping data\n",
    "BASE_URL = 'https://www.tradingview.com/symbols/IDX-'\n",
    "TECHNICAL_ENUM = ['sell', 'neutral', 'buy']\n",
    "ANALYST_ENUM = ['strong_buy', 'buy', 'hold', 'sell', 'strong_sell']\n",
    "\n",
    "def get_url_page(symbol:str) -> str:\n",
    "    return f\"{BASE_URL}{symbol}\"\n",
    "\n",
    "# async def fetch(session, url):\n",
    "#     try:\n",
    "#       async with session.get(url) as response:\n",
    "#           return await response.text()\n",
    "#     except:\n",
    "#       print(f'Fail to fetch from {url}')\n",
    "#       return None\n",
    "\n",
    "# async def parse_technical(html) -> dict:\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "#     # Getting into the data\n",
    "#     speedometer_containers = soup.findAll(\"div\", {\"class\": \"speedometerWrapper-kg4MJrFB\"})\n",
    "#     summary_technical_data_wrapper = speedometer_containers[1]\n",
    "#     technical_counters_data_wrapper = summary_technical_data_wrapper.findAll(\"div\", {\"class\": \"counterWrapper-kg4MJrFB\"})\n",
    "\n",
    "#     technical_number_data = []\n",
    "#     for technical_counter in technical_counters_data_wrapper:\n",
    "#       # Get the number data\n",
    "#       technical_counters_number = technical_counter.find(\"span\", {\"class\": \"counterNumber-kg4MJrFB\"})\n",
    "#       print(technical_counters_number)\n",
    "    \n",
    "#     return dict()\n",
    "\n",
    "# async def scrap(symbol: str):\n",
    "#     url = get_url_page(symbol)\n",
    "#     technical_url = url+\"/technicals/\"\n",
    "#     async with aiohttp.ClientSession() as session:\n",
    "#         html = await fetch(session, technical_url)\n",
    "#         if (html is not None):\n",
    "#           links = await parse_technical(html)\n",
    "#           # Do something with the results\n",
    "#           print(links)\n",
    "\n",
    "# await scrap(\"GOTO\")\n",
    "\n",
    "\n",
    "def scrap_page(url: str) :\n",
    "    try:\n",
    "      session = HTMLSession()\n",
    "      response = session.get(url)\n",
    "      response.html.render()\n",
    "\n",
    "      soup = BeautifulSoup(response.html.html, \"html.parser\")\n",
    "      return soup\n",
    "    except Exception as e:\n",
    "      print(f\"Fail scraping from URL: {url}\")\n",
    "      print(e)\n",
    "      return None\n",
    "    \n",
    "\n",
    "def save_to_json(file_path, data):\n",
    "  with open(file_path, \"w\") as output_file:\n",
    "    json.dump(data, output_file, indent=2)\n",
    "\n",
    "def scrap_rating_data(symbol: str) -> dict:\n",
    "    url = get_url_page(symbol)\n",
    "    result_data = dict()\n",
    "    result_data['symbol'] = symbol\n",
    "    technical_rating_dict = None\n",
    "    analyst_rating_dict = None\n",
    "\n",
    "    # Scrap technical page\n",
    "    technical_url = url+\"/technicals/\"\n",
    "    soup_tpage = scrap_page(technical_url)\n",
    "    if (soup_tpage is not None):\n",
    "      # Getting into the data\n",
    "      speedometer_containers = soup_tpage.findAll(\"div\", {\"class\": \"speedometerWrapper-kg4MJrFB\"})\n",
    "      summary_technical_data_wrapper = speedometer_containers[1]\n",
    "      technical_counters_data_wrapper = summary_technical_data_wrapper.findAll(\"div\", {\"class\": \"counterWrapper-kg4MJrFB\"})\n",
    "\n",
    "      technical_number_data = []\n",
    "      for technical_counter in technical_counters_data_wrapper:\n",
    "        # Get the number data\n",
    "        technical_counters_number = technical_counter.find(\"span\", {\"class\": \"counterNumber-kg4MJrFB\"})\n",
    "        print(technical_counters_number)\n",
    "        \n",
    "      \n",
    "      # # Getting technical\n",
    "      # if (item.text == \"More technicals\"):\n",
    "      #   technical_rating_dict = dict()\n",
    "\n",
    "      #   item.click()\n",
    "      #   try:\n",
    "      #     _ = WebDriverWait(driver, 5).until(\n",
    "      #         EC.presence_of_element_located((By.CLASS_NAME, \"speedometerWrapper-kg4MJrFB\"))\n",
    "      #       )\n",
    "      #     technical_data_wrapper = driver.find_elements(By.CLASS_NAME, \"speedometerWrapper-kg4MJrFB\")\n",
    "      #     assert (len(technical_data_wrapper) == 3), \"Difference in technical data wrapper detected\"\n",
    "\n",
    "      #     # Summary should be the middle one\n",
    "      #     summary_technical_data_wrapper = technical_data_wrapper[1]\n",
    "      #     technical_counters_data_wrapper = summary_technical_data_wrapper.find_element(By.CLASS_NAME, \"countersWrapper-kg4MJrFB\")\n",
    "      #     technical_rating_data = technical_counters_data_wrapper.text.split(\"\\n\")\n",
    "\n",
    "      #     # Insert the data to dictionary\n",
    "      #     start_rating_data_idx = 1\n",
    "      #     for enum in TECHNICAL_ENUM:\n",
    "      #       technical_rating_dict[enum] = int(technical_rating_data[start_rating_data_idx])\n",
    "      #       start_rating_data_idx +=2\n",
    "        \n",
    "      #     technical_rating_dict['updated_on'] = (datetime.now()).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # # Scrap forecast page (analyst)\n",
    "    # forecast_url = url+\"/forecast/\"\n",
    "    # soup_fpage = scrap_page(forecast_url)\n",
    "    #       except:\n",
    "    #         print(\"Failed to get Technical Data\")\n",
    "\n",
    "    #     # Getting Analyst Rating\n",
    "    #     if (item.text == \"See forecast\"):\n",
    "    #       analyst_rating_dict = dict()\n",
    "          \n",
    "\n",
    "    #       item.click()\n",
    "\n",
    "    #       try:\n",
    "    #         _ = WebDriverWait(driver, 10).until(\n",
    "    #             EC.presence_of_element_located((By.CLASS_NAME, \"container-zZSa1SHt\"))\n",
    "    #           )\n",
    "    #         analyst_data_wrapper = driver.find_element(By.CLASS_NAME, \"container-zZSa1SHt\")\n",
    "\n",
    "    #         # Get the Value\n",
    "    #         analyst_data_values = analyst_data_wrapper.find_elements(By.CLASS_NAME,\"value-GNeDL9vy\")\n",
    "\n",
    "    #         # Insert the data to dictionary\n",
    "    #         for idx, enum in enumerate(ANALYST_ENUM):\n",
    "    #            analyst_rating_dict[enum] = int((analyst_data_values[idx]).text)\n",
    "\n",
    "    #         analyst_rating_dict['updated_on'] = (datetime.now()).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    #       except:\n",
    "    #         print(\"Failed to get Analyst Data\")\n",
    "\n",
    "    result_data['technical_rating'] = technical_rating_dict\n",
    "    result_data['analyst_rating'] = analyst_rating_dict\n",
    "    return result_data\n",
    "\n",
    "scrap_rating_data(\"GOTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrap_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m i2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m i1\n\u001b[0;32m      5\u001b[0m i3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m i1\n\u001b[1;32m----> 7\u001b[0m p1 \u001b[38;5;241m=\u001b[39m Process(target\u001b[38;5;241m=\u001b[39m\u001b[43mscrap_function\u001b[49m, args\u001b[38;5;241m=\u001b[39m(symbol_list[:i1], \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m      8\u001b[0m p2 \u001b[38;5;241m=\u001b[39m Process(target\u001b[38;5;241m=\u001b[39mscrap_function, args\u001b[38;5;241m=\u001b[39m(symbol_list[i1:i2], \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m      9\u001b[0m p3 \u001b[38;5;241m=\u001b[39m Process(target\u001b[38;5;241m=\u001b[39mscrap_function, args\u001b[38;5;241m=\u001b[39m(symbol_list[i2:i3], \u001b[38;5;241m3\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scrap_function' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  length_list = len(symbol_list)\n",
    "  i1 = int(length_list / 4)\n",
    "  i2 = 2 * i1\n",
    "  i3 = 3 * i1\n",
    "\n",
    "  p1 = Process(target=scrap_function, args=(symbol_list[:i1], 1))\n",
    "  p2 = Process(target=scrap_function, args=(symbol_list[i1:i2], 2))\n",
    "  p3 = Process(target=scrap_function, args=(symbol_list[i2:i3], 3))\n",
    "  p4 = Process(target=scrap_function, args=(symbol_list[i3:], 4))\n",
    "\n",
    "  p1.start()\n",
    "  p2.start()\n",
    "  p3.start()\n",
    "  p4.start()\n",
    "\n",
    "  p1.join()\n",
    "  p2.join()\n",
    "  p3.join()\n",
    "  p4.join()\n",
    "\n",
    "  # Merge and upsert to db\n",
    "  df_merge = combine_data(df_db_data)\n",
    "\n",
    "  # Convert to json. Remove the index in dataframe\n",
    "  records = df_merge.to_dict(orient=\"records\")\n",
    "\n",
    "  # Upsert to db\n",
    "  try:\n",
    "    supabase.table(\"idx_key_stats\").upsert(\n",
    "        records\n",
    "    ).execute()\n",
    "    print(\n",
    "        f\"Successfully upserted {len(records)} data to database\"\n",
    "    )\n",
    "  except Exception as e:\n",
    "    raise Exception(f\"Error upserting to database: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
